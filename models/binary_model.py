# -*- coding: utf-8 -*-
"""Binary-Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dxpExkfeMmSv-nlaN1__kcabM_LPxuq9
"""

from google.colab import drive
drive.mount('/content/drive')

base_dir = '/content/drive/My Drive/Data'

shape_file = base_dir + '/labels/shape/shape_anno_all.txt'

shape_columns = {
    "sleeve_length": {0: "sleeveless", 1: "short-sleeve", 2: "medium-sleeve", 3: "long-sleeve", 4: "not long-sleeve", 5: "NA"},
    "lower_clothing_length": {0: "three-point", 1: "medium short", 2: "three-quarter", 3: "long", 4: "NA"},
    "socks": {0: "no socks", 1: "socks", 2: "leggings", 3: "NA"},
    "hat": {0: "no hat", 1: "yes hat", 2: "NA"},
    "glasses": {0: "no glasses", 1: "eyeglasses", 2: "sunglasses", 3: "glasses in hand/clothes", 4: "NA"},
    "neckwear": {0: "no neckwear", 1: "yes neckwear", 2: "NA"},
    "wrist_wearing": {0: "no wristwear", 1: "yes wristwear", 2: "NA"},
    "ring": {0: "no ring", 1: "yes ring", 2: "NA"},
    "waist_accessories": {0: "no waist accessories", 1: "belt", 2: "have clothing", 3: "hidden", 4: "NA"},
    "neckline": {0: "V-shape", 1: "square", 2: "round", 3: "standing", 4: "lapel", 5: "suspenders", 6: "NA"},
    "outer_clothing_cardigan": {0: "yes cardigan", 1: "no cardigan", 2: "NA"},
    "upper_clothing_covering_navel": {0: "no", 1: "yes",2:"NA"}
}

import pandas as pd
shape_data = pd.read_csv(shape_file, delim_whitespace=True, header=None)
shape_data.columns = ["image_name"] + list(shape_columns.keys())

from sklearn.model_selection import train_test_split
binary_data = shape_data[['image_name','hat','neckwear','outer_clothing_cardigan','upper_clothing_covering_navel']]

binary_data.head()

from sklearn.utils import resample
target_count = 2500

def resample_class(df, column, target_count=2500):
    """Resample each class to have the same number of samples."""
    classes = []
    for class_value in df[column].unique():
        class_df = df[df[column] == class_value]
        resampled_class = resample(
            class_df,
            replace=True,  # Oversample if needed
            n_samples=target_count,
            random_state=42
        )
        classes.append(resampled_class)
    return pd.concat(classes)

resampled_data = []
for column in ["hat", "neckwear", "outer_clothing_cardigan", "upper_clothing_covering_navel"]:
    resampled_data.append(resample_class(binary_data, column, target_count))

# Combine and drop duplicates
resampled_df = pd.concat(resampled_data).drop_duplicates()

import os
image_dir = '/content/drive/My Drive/Data/images/'
resampled_df['image_path'] = resampled_df['image_name'].apply(lambda x: os.path.join(image_dir, x))

# Filter out rows where the image file does not exist
resampled_df = resampled_df[resampled_df['image_path'].apply(os.path.exists)]

# Check the number of valid rows
print(f"Number of valid images: {len(resampled_df)}")

# Drop duplicates (if needed) and reset index
resampled_df = resampled_df.drop_duplicates().reset_index(drop=True)

train_df, val_df = train_test_split(resampled_df, test_size=0.2, random_state=42)

IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

# Helper function to preprocess data
def preprocess_data(df):
    filepaths = df["image_path"].tolist()
    labels = {
        "hat": df["hat"].values,
        "neckwear": df["neckwear"].values,
        "outer_clothing_cardigan": df["outer_clothing_cardigan"].values,
        "upper_clothing_covering_navel": df["upper_clothing_covering_navel"].values,
    }
    return filepaths, labels

train_filepaths, train_labels = preprocess_data(train_df)
val_filepaths, val_labels = preprocess_data(val_df)

import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input

def data_generator(filepaths, labels, batch_size):
    def generator():
        for idx, filepath in enumerate(filepaths):
            # Read image
            img = tf.image.decode_jpeg(tf.io.read_file(filepath))
            img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH]) / 255.0  # Normalize
            yield img, {key: labels[key][idx] for key in labels}

    dataset = tf.data.Dataset.from_generator(
        generator,
        output_signature=(
            tf.TensorSpec(shape=(IMG_HEIGHT, IMG_WIDTH, 3), dtype=tf.float32),
            {key: tf.TensorSpec(shape=(), dtype=tf.int32) for key in labels}
        )
    )
    return dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)

train_data = data_generator(train_filepaths, train_labels, BATCH_SIZE)
val_data = data_generator(val_filepaths, val_labels, BATCH_SIZE)

input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)
base_model = ResNet50(weights="imagenet", include_top=False, input_tensor=Input(shape=input_shape))
x = GlobalAveragePooling2D()(base_model.output)

# Define output layers for each attribute
hat_output = Dense(3, activation="softmax", name="hat")(x)
neckwear_output = Dense(3, activation="softmax", name="neckwear")(x)
outer_clothing_cardigan_output = Dense(3, activation="softmax", name="outer_clothing_cardigan")(x)
upper_clothing_covering_navel_output = Dense(3, activation="softmax", name="upper_clothing_covering_navel")(x)

model = Model(
    inputs=base_model.input,
    outputs=[
        hat_output,
        neckwear_output,
        outer_clothing_cardigan_output,
        upper_clothing_covering_navel_output,
    ]
)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss={
        "hat": "sparse_categorical_crossentropy",
        "neckwear": "sparse_categorical_crossentropy",
        "outer_clothing_cardigan": "sparse_categorical_crossentropy",
        "upper_clothing_covering_navel": "sparse_categorical_crossentropy",
    },
    metrics={
        "hat": "accuracy",
        "neckwear": "accuracy",
        "outer_clothing_cardigan": "accuracy",
        "upper_clothing_covering_navel": "accuracy",
    },
)

model.summary()

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=10,
    steps_per_epoch=len(train_filepaths) // BATCH_SIZE,
    validation_steps=len(val_filepaths) // BATCH_SIZE,
)

from sklearn.metrics import classification_report

# Get predictions from the model
predictions = model.predict(val_data)
predicted_labels = {
    "hat": predictions[0].argmax(axis=-1),
    "neckwear": predictions[1].argmax(axis=-1),
    "outer_clothing_cardigan": predictions[2].argmax(axis=-1),
    "upper_clothing_covering_navel": predictions[3].argmax(axis=-1),
}

# True labels
true_labels = {
    "hat": val_df["hat"].values,
    "neckwear": val_df["neckwear"].values,
    "outer_clothing_cardigan": val_df["outer_clothing_cardigan"].values,
    "upper_clothing_covering_navel": val_df["upper_clothing_covering_navel"].values,
}

# Classification report for each attribute
for key in predicted_labels:
    print(f"\nClassification Report for {key}:")
    print(classification_report(true_labels[key], predicted_labels[key], digits=4))

# Predictions on train and test data
train_predictions = model.predict(train_data)
val_predictions = model.predict(val_data)

# Extract true labels for train and test datasets
train_true_labels = {
    "hat": train_df["hat"].values,
    "neckwear": train_df["neckwear"].values,
    "outer_clothing_cardigan": train_df["outer_clothing_cardigan"].values,
    "upper_clothing_covering_navel": train_df["upper_clothing_covering_navel"].values,
}

val_true_labels = {
    "hat": val_df["hat"].values,
    "neckwear": val_df["neckwear"].values,
    "outer_clothing_cardigan": val_df["outer_clothing_cardigan"].values,
    "upper_clothing_covering_navel": val_df["upper_clothing_covering_navel"].values,
}

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt


# Function to plot confusion matrix
def plot_confusion_matrix(true_labels, predictions, attribute_name):
    cm = confusion_matrix(true_labels, predictions)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="Blues")
    disp.ax_.set_title(f"Confusion Matrix: {attribute_name}")
    disp.ax_.set_xlabel("Predicted Labels")
    disp.ax_.set_ylabel("True Labels")
    plt.show()

from sklearn.metrics import roc_curve, auc

# Function to plot ROC curve for an attribute
def plot_roc_curve(true_labels, probabilities, attribute_name):
    plt.figure(figsize=(8, 6))
    for class_idx in range(probabilities.shape[1]):  # Loop through each class
        fpr, tpr, _ = roc_curve(true_labels == class_idx, probabilities[:, class_idx])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"Class {class_idx} (AUC = {roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], "k--")  # Diagonal line for random chance
    plt.title(f"ROC Curve: {attribute_name}")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")
    plt.show()

# Confusion Matrix for Train Data
for attribute in train_true_labels.keys():
    predicted_labels = train_predictions[list(train_true_labels.keys()).index(attribute)].argmax(axis=-1)
    plot_confusion_matrix(train_true_labels[attribute], predicted_labels, f"{attribute} (Train)")

# ROC Curve for Train Data
for attribute in train_true_labels.keys():
    probabilities = train_predictions[list(train_true_labels.keys()).index(attribute)]
    plot_roc_curve(train_true_labels[attribute], probabilities, f"{attribute} (Train)")

# Confusion Matrix for Validation Data
for attribute in val_true_labels.keys():
    predicted_labels = val_predictions[list(val_true_labels.keys()).index(attribute)].argmax(axis=-1)
    plot_confusion_matrix(val_true_labels[attribute], predicted_labels, f"{attribute} (Validation)")

# ROC Curve for Validation Data
for attribute in val_true_labels.keys():
    probabilities = val_predictions[list(val_true_labels.keys()).index(attribute)]
    plot_roc_curve(val_true_labels[attribute], probabilities, f"{attribute} (Validation)")

model.save("multi_attribute_classifier.h5")